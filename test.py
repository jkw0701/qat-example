import os
import argparse
import random
import time
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.quantization as quant
from torch.quantization import QuantStub, DeQuantStub
from collections import deque
import matplotlib.pyplot as plt
import seaborn as sns
import sys
from datetime import datetime
import copy

# ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

# Quantization backend ì„¤ì •
torch.backends.quantized.engine = 'qnnpack'

# í™˜ê²½ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
print(f"Quantization backend: {torch.backends.quantized.engine}")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° - ë” ê¸´ í›ˆë ¨ì„ ìœ„í•´ ì¡°ì •
ENV_NAME = "CartPole-v1"
BUFFER_SIZE = 10000
BATCH_SIZE = 64
GAMMA = 0.99
LR = 1e-3
EPS_START = 1.0
EPS_END = 0.01
EPS_DECAY = 0.995
TARGET_UPDATE = 100
TRAIN_EPISODES = 2000  # 300 -> 1000ìœ¼ë¡œ ì¦ê°€
EVAL_EPISODES = 100

MODEL_DIR = "models"
NORMAL_MODEL_PATH = os.path.join(MODEL_DIR, "dqn_normal.pth")
QAT_MODEL_PATH = os.path.join(MODEL_DIR, "dqn_qat.pth")
RESULTS_DIR = "results"

# ì¶”ê°€: ë” ì„¸ë°€í•œ ë¶„ì„ì„ ìœ„í•œ ì„¤ì •
EXTENDED_TRAIN_EPISODES = 2000  # í™•ì¥ í›ˆë ¨ìš©
DETAILED_EVAL_EPISODES = 500    # ë” ì •í™•í•œ í‰ê°€ìš©

# ğŸ” 1ë‹¨ê³„: QAT ë¬¸ì œ ì§„ë‹¨ì„ ìœ„í•œ ë””ë²„ê¹… ë„êµ¬ë“¤
class QATDiagnostics:
    """QAT ê´€ë ¨ ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ëŠ” ë„êµ¬"""
    
    @staticmethod
    def check_pytorch_version():
        """PyTorch ë²„ì „ í™•ì¸"""
        print(f"PyTorch ë²„ì „: {torch.__version__}")
        print(f"Quantization ì§€ì›: {hasattr(torch, 'quantization')}")
        
        # ê¶Œì¥ ë²„ì „ í™•ì¸
        version_parts = torch.__version__.split('.')
        major, minor = int(version_parts[0]), int(version_parts[1])
        
        if major >= 1 and minor >= 8:
            print("âœ… QAT ì§€ì› ë²„ì „ì…ë‹ˆë‹¤.")
        else:
            print("âš ï¸  PyTorch 1.8+ ë²„ì „ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
    
    @staticmethod
    def diagnose_qat_model(model, model_name="QAT Model"):
        """QAT ëª¨ë¸ ìƒíƒœ ì§„ë‹¨"""
        print(f"\n=== {model_name} ì§„ë‹¨ ===")
        
        # 1. ëª¨ë¸ ëª¨ë“œ í™•ì¸
        print(f"í›ˆë ¨ ëª¨ë“œ: {model.training}")
        
        # 2. íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params}/{total_params}")
        
        # 3. QAT íŠ¹í™” í™•ì¸
        has_fake_quant = False
        has_observer = False
        
        for name, module in model.named_modules():
            module_type = type(module).__name__
            if 'FakeQuantize' in module_type:
                has_fake_quant = True
                print(f"  FakeQuantize ë°œê²¬: {name} -> {module_type}")
            elif 'Observer' in module_type:
                has_observer = True
                print(f"  Observer ë°œê²¬: {name} -> {module_type}")
        
        print(f"FakeQuantize ëª¨ë“ˆ: {'ìˆìŒ' if has_fake_quant else 'ì—†ìŒ'}")
        print(f"Observer ëª¨ë“ˆ: {'ìˆìŒ' if has_observer else 'ì—†ìŒ'}")
        
        # 4. qconfig í™•ì¸
        if hasattr(model, 'qconfig') and model.qconfig:
            print(f"qconfig: {model.qconfig}")
        else:
            print("qconfig: ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        # 5. ê° íŒŒë¼ë¯¸í„°ì˜ gradient ìƒíƒœ
        print("íŒŒë¼ë¯¸í„° gradient ìƒíƒœ:")
        for name, param in model.named_parameters():
            print(f"  {name}: requires_grad={param.requires_grad}, "
                  f"grad={'ìˆìŒ' if param.grad is not None else 'ì—†ìŒ'}, "
                  f"device={param.device}")
    
    @staticmethod
    def test_forward_backward(model, input_shape, device='cpu'):
        """Forward/Backward íŒ¨ìŠ¤ í…ŒìŠ¤íŠ¸"""
        print(f"\n=== Forward/Backward í…ŒìŠ¤íŠ¸ ===")
        
        model = model.to(device)
        model.train()
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        x = torch.randn(2, *input_shape, device=device)
        target = torch.randn(2, 2, device=device)  # 2ê°œ ì¶œë ¥ ê°€ì •
        
        # Forward íŒ¨ìŠ¤
        try:
            output = model(x)
            print(f"âœ… Forward ì„±ê³µ: {output.shape}")
        except Exception as e:
            print(f"âŒ Forward ì‹¤íŒ¨: {e}")
            return False
        
        # Loss ê³„ì‚°
        loss = F.mse_loss(output, target)
        print(f"Loss: {loss.item():.6f}")
        
        # Backward íŒ¨ìŠ¤
        try:
            loss.backward()
            print("âœ… Backward ì„±ê³µ")
            
            # Gradient í™•ì¸
            grad_count = 0
            for name, param in model.named_parameters():
                if param.grad is not None and param.grad.norm() > 0:
                    grad_count += 1
                    print(f"  {name}: grad_norm={param.grad.norm().item():.8f}")
            
            if grad_count > 0:
                print(f"âœ… {grad_count}ê°œ íŒŒë¼ë¯¸í„°ì— gradient ì¡´ì¬")
                return True
            else:
                print("âŒ Gradientê°€ ì—†ìŠµë‹ˆë‹¤!")
                return False
                
        except Exception as e:
            print(f"âŒ Backward ì‹¤íŒ¨: {e}")
            return False

# ì¼ë°˜ DQN ë„¤íŠ¸ì›Œí¬
class DQN(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=128):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)

# ì˜¬ë°”ë¥¸ Quantization-Aware Trainingì„ ìœ„í•œ DQN
class QAT_DQN(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=128):
        super(QAT_DQN, self).__init__()
        # Quantization stubs
        self.quant = QuantStub()
        self.dequant = DeQuantStub()
        
        # ë ˆì´ì–´ë“¤ì„ ë³„ë„ ëª¨ë“ˆë¡œ ì •ì˜ (fuse_modulesë¥¼ ìœ„í•´)
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.relu1 = nn.ReLU(inplace=False)  # inplace=Falseê°€ ì¤‘ìš”!
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.relu2 = nn.ReLU(inplace=False)
        self.fc3 = nn.Linear(hidden_size, action_size)
        self._initialize_weights()

    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        x = self.quant(x)      # ì…ë ¥ quantization
        x = self.fc1(x)        # fc1 ê°€ì¤‘ì¹˜ fake quantization
        x = self.relu1(x)      # relu1 activation fake quantization
        x = self.fc2(x)        # fc2 ê°€ì¤‘ì¹˜ fake quantization
        x = self.relu2(x)      # relu2 activation fake quantization
        x = self.fc3(x)        # fc3 ê°€ì¤‘ì¹˜ fake quantization
        x = self.dequant(x)    # ì¶œë ¥ dequantization
        return x
        
    def fuse_model(self):
        """ë ˆì´ì–´ ìœµí•© (Linear + ReLU)"""
        # fc1ê³¼ relu1 ìœµí•©
        torch.quantization.fuse_modules(self, [['fc1', 'relu1']], inplace=True)
        # fc2ì™€ relu2 ìœµí•©
        torch.quantization.fuse_modules(self, [['fc2', 'relu2']], inplace=True)

# Experience Replay Buffer
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)

# DQN Agent
class DQNAgent:
    def __init__(self, state_size, action_size, lr=1e-3, use_qat=False):
        self.state_size = state_size
        self.action_size = action_size
        self.use_qat = use_qat
        
        # ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
        if use_qat:
            self.q_network = QAT_DQN(state_size, action_size).to(device)
            self.target_network = QAT_DQN(state_size, action_size).to(device)
            
            # QAT ì„¤ì • - ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì ìš©
            # Step 1: ëª¨ë¸ ìœµí•© (Linear + ReLU)
            self.q_network.fuse_model()
            self.target_network.fuse_model()
            
            # Step 2: qconfig ì„¤ì • - ë” ì•ˆì •ì ì¸ ì„¤ì • ì‚¬ìš©
            self.q_network.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
            self.target_network.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
            
            # Step 3: QAT ì¤€ë¹„ - fake quantization í™œì„±í™”
            torch.quantization.prepare_qat(self.q_network, inplace=True)
            torch.quantization.prepare_qat(self.target_network, inplace=True)
            
            print("QAT ì„¤ì • ì™„ë£Œ:")
            print(f"  - ìœµí•©ëœ ëª¨ë“ˆ: fc1+relu1, fc2+relu2")
            print(f"  - qconfig: {self.q_network.qconfig}")
            print(f"  - fake quantization í™œì„±í™”ë¨")
        else:
            self.q_network = DQN(state_size, action_size).to(device)
            self.target_network = DQN(state_size, action_size).to(device)
        
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)
        self.memory = ReplayBuffer(10000)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.batch_size = 64
        self.gamma = 0.99
        self.epsilon = 0.3
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.995
        self.update_target_freq = 50
        self.step_count = 0
        
        # í›ˆë ¨ ê¸°ë¡ìš©
        self.training_scores = []
        self.training_losses = []
        self.epsilon_history = []
        
    def remember(self, state, action, reward, next_state, done):
        self.memory.push(state, action, reward, next_state, done)
    
    def act(self, state):
        if np.random.random() <= self.epsilon:
            return random.choice(np.arange(self.action_size))
        
        # torch.FloatTensor(state): NumPy ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë³€í™˜
        #.unsqueeze(0): ë°°ì¹˜ ì°¨ì› ì¶”ê°€ (shape: [4] â†’ [1, 4])
        #ì´ìœ : ì‹ ê²½ë§ì€ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸
        state = torch.FloatTensor(state).unsqueeze(0)
        
        # ëª¨ë¸ì´ quantizedëœ ê²½ìš° ë˜ëŠ” QAT ëª¨ë¸ì¸ ê²½ìš° CPUì—ì„œ ì‹¤í–‰
        is_quantized = (hasattr(self.q_network, '_modules') and 
                       any('quantized' in str(type(m)).lower() for m in self.q_network.modules()))
        
        if self.use_qat or is_quantized:
            # QAT ëª¨ë¸ì´ë‚˜ quantized ëª¨ë¸ì€ CPUì—ì„œ ì‹¤í–‰
            state = state.to('cpu')
            self.q_network = self.q_network.to('cpu')
        else:
            # ì¼ë°˜ ëª¨ë¸ì€ ì›ë˜ ë””ë°”ì´ìŠ¤ì—ì„œ ì‹¤í–‰
            state = state.to(device)
            self.q_network = self.q_network.to(device)
            
        with torch.no_grad():
            q_values = self.q_network(state)
            # .cpu() : ê²°ê³¼ë¥¼ CPUë¡œ ì´ë™, data.numpy() : PyTorch í…ì„œë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜
        return np.argmax(q_values.cpu().data.numpy()) # ìµœëŒ€ê°’ì˜ ì¸ë±ìŠ¤ ë°˜í™˜ (ìµœì  í–‰ë™)
     
    def replay(self):
        """í–¥ìƒëœ replay í•¨ìˆ˜ - í•™ìŠµ ìƒíƒœ ëª¨ë‹ˆí„°ë§"""
        if len(self.memory) < self.batch_size:
            return None
        
        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)
        
        target_device = 'cpu' if self.use_qat else device
        
        states = torch.FloatTensor(states).to(target_device)
        actions = torch.LongTensor(actions).to(target_device)
        rewards = torch.FloatTensor(rewards).to(target_device)
        next_states = torch.FloatTensor(next_states).to(target_device)
        dones = torch.BoolTensor(dones).to(target_device)
        
        self.q_network = self.q_network.to(target_device)
        self.target_network = self.target_network.to(target_device)
        
        # Forward pass
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.target_network(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # Loss ê³„ì‚°
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        # Gradient ê³„ì‚° ë° í™•ì¸
        self.optimizer.zero_grad()
        loss.backward()
        
        # Gradient norm í™•ì¸ (ë§¤ 100 ìŠ¤í…ë§ˆë‹¤)
        if self.step_count % 100 == 0:
            total_grad_norm = 0
            param_count = 0
            for name, param in self.q_network.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    total_grad_norm += grad_norm
                    param_count += 1
            
            #if param_count > 0:
            #    avg_grad_norm = total_grad_norm / param_count
            #    print(f"Step {self.step_count}: í‰ê·  gradient norm = {avg_grad_norm:.6f}")
        
        # Gradient clipping (ì•ˆì •ì„± í–¥ìƒ)
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        # Epsilon ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        self.step_count += 1
        if self.step_count % self.update_target_freq == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
        
        return loss.item()

    
    def benchmark_inference_speed(self, num_samples=1000):
        """ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬"""
        self.q_network.eval()
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥ ìƒì„±
        test_inputs = torch.randn(num_samples, self.state_size)
        
        # ëª¨ë¸ì´ quantizedë˜ì—ˆê±°ë‚˜ QAT ëª¨ë¸ì¸ì§€ í™•ì¸
        is_quantized = (hasattr(self.q_network, '_modules') and 
                       any('quantized' in str(type(m)).lower() for m in self.q_network.modules()))
        
        if self.use_qat or is_quantized:
            test_inputs = test_inputs.to('cpu')
            self.q_network = self.q_network.to('cpu')
            device_name = 'cpu'
        else:
            test_inputs = test_inputs.to(device)
            self.q_network = self.q_network.to(device)
            device_name = str(device)
        
        # ì›Œë°ì—…
        with torch.no_grad():
            for _ in range(10):
                _ = self.q_network(test_inputs[:10])
        
        # ì‹¤ì œ ì¸¡ì •
        if torch.cuda.is_available() and device_name != 'cpu':
            torch.cuda.synchronize()
        start_time = time.time()
        
        with torch.no_grad():
            for i in range(0, num_samples, 32):
                batch = test_inputs[i:i+32]
                _ = self.q_network(batch)
        
        if torch.cuda.is_available() and device_name != 'cpu':
            torch.cuda.synchronize()
        end_time = time.time()
        
        total_time = end_time - start_time
        avg_time_per_sample = (total_time / num_samples) * 1000
        throughput = num_samples / total_time
        
        return {
            'total_time': total_time,
            'avg_time_ms': avg_time_per_sample,
            'throughput': throughput,
            'is_quantized': is_quantized or self.use_qat,
            'device': device_name
        }
    
    def get_model_size_mb(self):
        """ëª¨ë¸ í¬ê¸° ê³„ì‚° (MB ë‹¨ìœ„)"""
        param_size = 0
        buffer_size = 0
        
        for param in self.q_network.parameters():
            param_size += param.nelement() * param.element_size()
        
        for buffer in self.q_network.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        model_size = param_size + buffer_size
        return model_size / (1024 * 1024)
    
    def analyze_weight_distribution(self):
        """ê°€ì¤‘ì¹˜ ë¶„í¬ ë¶„ì„"""
        weight_stats = {}
        
        for name, param in self.q_network.named_parameters():
            if 'weight' in name:
                weight_data = param.data.cpu().numpy().flatten()
                weight_stats[name] = {
                    'data': weight_data,
                    'min': float(np.min(weight_data)),
                    'max': float(np.max(weight_data)),
                    'mean': float(np.mean(weight_data)),
                    'std': float(np.std(weight_data)),
                    'unique_values': len(np.unique(weight_data)),
                    'dtype': str(param.dtype),
                    'shape': list(param.shape)
                }
        
        return weight_stats
    
    def save(self, path):
        os.makedirs(MODEL_DIR, exist_ok=True)
        if self.use_qat:
            # QAT ëª¨ë¸ì˜ ê²½ìš° eval ëª¨ë“œë¡œ ì „í™˜ í›„ convert
            self.q_network.eval()
            net_cpu = self.q_network.cpu()
            
            # ì‹¤ì œ quantized ëª¨ë¸ë¡œ ë³€í™˜
            try:
                net_quantized = torch.quantization.convert(net_cpu, inplace=False)
                print("QAT ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ quantized ëª¨ë¸ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # í›ˆë ¨ íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ì €ì¥
                torch.save({
                    'model_state_dict': net_quantized.state_dict(),
                    'training_scores': self.training_scores,
                    'training_losses': self.training_losses,
                    'epsilon_history': self.epsilon_history,
                    'use_qat': self.use_qat,
                    'is_quantized': True
                }, path)
                
                # ì›ë˜ ë””ë°”ì´ìŠ¤ë¡œ ë³µì› (í›ˆë ¨ ê³„ì†í•˜ëŠ” ê²½ìš°ë¥¼ ìœ„í•´)
                self.q_network = self.q_network.to(device)
                self.q_network.train()
                
            except Exception as e:
                print(f"Quantization ë³€í™˜ ì‹¤íŒ¨: {e}")
                print("QAT ëª¨ë¸ì„ float ìƒíƒœë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
                
                torch.save({
                    'model_state_dict': net_cpu.state_dict(),
                    'training_scores': self.training_scores,
                    'training_losses': self.training_losses,
                    'epsilon_history': self.epsilon_history,
                    'use_qat': self.use_qat,
                    'is_quantized': False
                }, path)
        else:
            # ì¼ë°˜ ëª¨ë¸
            torch.save({
                'model_state_dict': self.q_network.state_dict(),
                'training_scores': self.training_scores,
                'training_losses': self.training_losses,
                'epsilon_history': self.epsilon_history,
                'use_qat': self.use_qat,
                'is_quantized': False
            }, path)
    
    
    def load_pretrained_weights(self, pretrained_state_dict):
        """ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì „ì´"""
        print("\nğŸ”„ ê°€ì¤‘ì¹˜ ì „ì´ ì‹œì‘...")
        
        # 1. ì„ì‹œ QAT ëª¨ë¸ ìƒì„± (ìœµí•© ì „)
        temp_qat = QAT_DQN(self.state_size, self.action_size).to('cpu')
        temp_qat.train()
        
        # 2. ê°€ì¤‘ì¹˜ ë³µì‚¬ (ìœµí•© ì „ì— ìˆ˜í–‰)
        print("ê°€ì¤‘ì¹˜ ë³µì‚¬ ì¤‘...")
        mapping = {
            'fc1.weight': 'fc1.weight',
            'fc1.bias': 'fc1.bias', 
            'fc2.weight': 'fc2.weight',
            'fc2.bias': 'fc2.bias',
            'fc3.weight': 'fc3.weight',
            'fc3.bias': 'fc3.bias'
        }
        
        for qat_key, normal_key in mapping.items():
            if normal_key in pretrained_state_dict:
                temp_qat.state_dict()[qat_key].copy_(pretrained_state_dict[normal_key])
                print(f"  {normal_key} -> {qat_key} âœ…")
        
        # 3. ì „ì´ ê²€ì¦ (ìœµí•© ì „ í…ŒìŠ¤íŠ¸)
        temp_qat.eval()
        test_input = torch.randn(1, self.state_size)
        with torch.no_grad():
            output_before = temp_qat(test_input)
        print(f"ìœµí•© ì „ ì¶œë ¥: {output_before}")
        
        # 4. QAT ì„¤ì • ì ìš©
        temp_qat.train()
        temp_qat.fuse_model()
        temp_qat.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
        torch.quantization.prepare_qat(temp_qat, inplace=True)
        
        # 5. QAT ì„¤ì • í›„ í…ŒìŠ¤íŠ¸
        temp_qat.eval()
        with torch.no_grad():
            output_after = temp_qat(test_input)
        print(f"QAT ì„¤ì • í›„ ì¶œë ¥: {output_after}")
        print(f"ì¶œë ¥ ì°¨ì´: {torch.norm(output_after - output_before).item():.6f}")
        
        # 6. ìµœì¢… ëª¨ë¸ êµì²´
        temp_qat.train()
        self.q_network = temp_qat
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # 7. Optimizer ì¬ìƒì„± (ì¤‘ìš”!)
        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=5e-4)
        
        print("âœ… ê°€ì¤‘ì¹˜ ì „ì´ ì™„ë£Œ!")
        
        # 8. ì „ì´ í›„ ì§„ë‹¨
        QATDiagnostics.diagnose_qat_model(self.q_network, "ê°€ì¤‘ì¹˜ ì „ì´ í›„")
    
    def set_qat_training_mode(self, phase="fine_tune"):
        """ê°œì„ ëœ QAT í›ˆë ¨ ëª¨ë“œ ì„¤ì •"""
        if phase == "fine_tune":
            # Fine-tuning: ì ì ˆí•œ íƒí—˜ê³¼ ì‘ì€ learning rate
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = 5e-4  # ë” ì‘ì€ LR
            self.epsilon = 0.3  # ì ì ˆí•œ íƒí—˜ ìœ ì§€
            self.epsilon_decay = 0.998  # ì²œì²œíˆ ê°ì†Œ
            self.epsilon_min = 0.05  # ìµœì†Œê°’ë„ ì¡°ì •
            print(f"QAT Fine-tuning ëª¨ë“œ ì„¤ì •: LR={5e-4}, Îµ={0.3}")
        
        elif phase == "normal":
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = 1e-3
            self.epsilon = 1.0
            self.epsilon_decay = 0.995
            self.epsilon_min = 0.01
            print("ì¼ë°˜ í›ˆë ¨ ëª¨ë“œë¡œ ë³µì›")

    def warmup_replay_buffer(self, env, episodes=50):
        """Replay buffer ì‚¬ì „ ì±„ìš°ê¸°"""
        print(f"ğŸ”„ Replay buffer ì›Œë°ì—… ì‹œì‘ ({episodes} ì—í”¼ì†Œë“œ)...")
        
        # ì„ì‹œë¡œ ë†’ì€ epsilon ì„¤ì • (ë‹¤ì–‘í•œ ê²½í—˜ ìˆ˜ì§‘)
        original_epsilon = self.epsilon
        self.epsilon = 0.8
        
        for episode in range(episodes):
            obs = env.reset()
            state = obs[0] if isinstance(obs, tuple) else obs
            
            while True:
                action = self.act(state)
                step_result = env.step(action)
                
                if len(step_result) == 5:
                    next_state, reward, terminated, truncated, info = step_result
                    done = terminated or truncated
                else:
                    next_state, reward, done, info = step_result
                
                self.remember(state, action, reward, next_state, done)
                state = next_state
                
                if done:
                    break
        
        self.epsilon = original_epsilon
        print(f"âœ… Buffer í¬ê¸°: {len(self.memory)}")

    
    def load(self, path):
        try:
            checkpoint = torch.load(path, map_location='cpu')
            
            # ìƒˆë¡œìš´ í˜•ì‹ì˜ ì²´í¬í¬ì¸íŠ¸ì¸ì§€ í™•ì¸
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                is_quantized = checkpoint.get('is_quantized', False)
                
                if self.use_qat:
                    if is_quantized:
                        # ì´ë¯¸ quantizedëœ ëª¨ë¸ ë¡œë“œ
                        print("Quantized ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
                        net_fp = QAT_DQN(self.state_size, self.action_size)
                        net_fp.fuse_model()
                        net_fp.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
                        torch.quantization.prepare_qat(net_fp, inplace=True)
                        net_q = torch.quantization.convert(net_fp.eval(), inplace=False)
                        net_q.load_state_dict(checkpoint['model_state_dict'])
                        self.q_network = net_q.to('cpu')
                    else:
                        # QAT ëª¨ë¸ì´ì§€ë§Œ ì•„ì§ quantizedë˜ì§€ ì•Šì€ ê²½ìš°
                        print("QAT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
                        self.q_network = QAT_DQN(self.state_size, self.action_size)
                        self.q_network.fuse_model()
                        self.q_network.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
                        torch.quantization.prepare_qat(self.q_network, inplace=True)
                        self.q_network.load_state_dict(checkpoint['model_state_dict'])
                        self.q_network = self.q_network.to('cpu')
                else:
                    # ì¼ë°˜ ëª¨ë¸
                    self.q_network = DQN(self.state_size, self.action_size)
                    self.q_network.load_state_dict(checkpoint['model_state_dict'])
                    self.q_network = self.q_network.to(device)
                
                # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ
                self.training_scores = checkpoint.get('training_scores', [])
                self.training_losses = checkpoint.get('training_losses', [])
                self.epsilon_history = checkpoint.get('epsilon_history', [])
                
            else:
                # ê¸°ì¡´ í˜•ì‹ (state_dictë§Œ ì €ì¥ëœ ê²½ìš°)
                print(f"ê¸°ì¡´ í˜•ì‹ì˜ ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤: {path}")
                if self.use_qat:
                    self.q_network = QAT_DQN(self.state_size, self.action_size)
                    self.q_network.fuse_model()
                    self.q_network.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
                    torch.quantization.prepare_qat(self.q_network, inplace=True)
                    # ê¸°ì¡´ í˜•ì‹ì€ quantizedë˜ì§€ ì•Šì€ ìƒíƒœë¡œ ê°€ì •
                    self.q_network.load_state_dict(checkpoint)
                    self.q_network = self.q_network.to('cpu')
                else:
                    self.q_network = DQN(self.state_size, self.action_size)
                    self.q_network.load_state_dict(checkpoint)
                    self.q_network = self.q_network.to(device)
                
                # ê¸°ì¡´ í˜•ì‹ì—ì„œëŠ” í›ˆë ¨ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŒ
                self.training_scores = []
                self.training_losses = []
                self.epsilon_history = []
                print("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ì—ëŠ” í›ˆë ¨ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            self.q_network.eval()
            
            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë„ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì„¤ì •
            if hasattr(self, 'target_network'):
                if self.use_qat:
                    self.target_network = self.target_network.to('cpu')
                else:
                    self.target_network = self.target_network.to(device)
            
            print(f"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {path}")
            print(f"ëª¨ë¸ ë””ë°”ì´ìŠ¤: {'CPU (QAT/Quantized)' if self.use_qat else device}")
            print(f"Quantized ìƒíƒœ: {checkpoint.get('is_quantized', 'Unknown')}")
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ëª¨ë¸ íŒŒì¼ì„ ë‹¤ì‹œ í›ˆë ¨í•´ì£¼ì„¸ìš”.")
            raise

def improved_transfer_learn_qat(normal_model_path, state_size, action_size, env, episodes=500):
    """ê°œì„ ëœ QAT ì „ì´ í•™ìŠµ"""
    
    print("\n" + "="*60)
    print("ğŸ”„ ê°œì„ ëœ QAT ì „ì´ í•™ìŠµ ì‹œì‘")
    print("="*60)
    
    # 1. Normal ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦
    print("1. ì‚¬ì „ í›ˆë ¨ëœ Normal DQN ë¡œë“œ ì¤‘...")
    normal_agent = DQNAgent(state_size, action_size, use_qat=False)
    
    try:
        normal_agent.load(normal_model_path)
        normal_scores = evaluate_agent(normal_agent, env, episodes=50)
        normal_avg = np.mean(normal_scores)
        print(f"   ğŸ“ˆ Normal ëª¨ë¸ í‰ê·  ì ìˆ˜: {normal_avg:.2f}")
        
        if normal_avg < 200:
            print(f"   âš ï¸  ê²½ê³ : Normal ëª¨ë¸ ì„±ëŠ¥ì´ ë‚®ìŠµë‹ˆë‹¤ ({normal_avg:.2f})")
            print("   ë” ì˜¤ë˜ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"   âŒ Normal ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    
    # 2. QAT ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ì „ì´
    print("\n2. QAT ëª¨ë¸ ìƒì„± ë° ê°œì„ ëœ ê°€ì¤‘ì¹˜ ì „ì´...")
    qat_agent = DQNAgent(state_size, action_size, use_qat=True)
    qat_agent.load_pretrained_weights(normal_agent.q_network.state_dict())
    
    # 3. Replay buffer ì›Œë°ì—…
    print("\n3. Replay buffer ì›Œë°ì—…...")
    qat_agent.warmup_replay_buffer(env, episodes=100)
    
    # 4. QAT Fine-tuning ì„¤ì •
    print("\n4. QAT Fine-tuning ì„¤ì •...")
    qat_agent.set_qat_training_mode("fine_tune")
    
    # 5. ì´ˆê¸° ì„±ëŠ¥ í™•ì¸ (ì›Œë°ì—… í›„)
    print("\n5. ì›Œë°ì—… í›„ ì´ˆê¸° ì„±ëŠ¥ í™•ì¸...")
    initial_scores = evaluate_agent(qat_agent, env, episodes=50)
    initial_avg = np.mean(initial_scores)
    print(f"   ğŸ“ˆ ì›Œë°ì—… í›„ QAT ì ìˆ˜: {initial_avg:.2f}")
    print(f"   ğŸ“Š ì„±ëŠ¥ ìœ ì§€ìœ¨: {(initial_avg/normal_avg)*100:.1f}%")
    
    # 6. í•™ìŠµ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    print(f"\n6. QAT Fine-tuning ì‹œì‘ ({episodes} ì—í”¼ì†Œë“œ)...")
    
    # ê°€ì¤‘ì¹˜ ë³€í™” ì¶”ì 
    initial_weight = qat_agent.q_network.fc1.weight.data.clone()
    weight_changes = []
    performance_checks = []
    
    # 7. ì‹¤ì œ Fine-tuning (ê°œì„ ëœ í›ˆë ¨ ë£¨í”„)
    scores = []
    scores_window = deque(maxlen=100)
    
    for episode in range(episodes):
        obs = env.reset()
        state = obs[0] if isinstance(obs, tuple) else obs
        total_reward = 0
        episode_losses = []
        
        while True:
            action = qat_agent.act(state)
            step_result = env.step(action)
            
            if len(step_result) == 5:
                next_state, reward, terminated, truncated, info = step_result
                done = terminated or truncated
            else:
                next_state, reward, done, info = step_result
            
            qat_agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            
            if done:
                break
            
            # í–¥ìƒëœ replay ì‚¬ìš©
            loss = qat_agent.replay()
            if loss is not None:
                episode_losses.append(loss)
        
        scores_window.append(total_reward)
        scores.append(total_reward)
        qat_agent.training_scores.append(total_reward)
        qat_agent.epsilon_history.append(qat_agent.epsilon)
        
        if episode_losses:
            qat_agent.training_losses.append(np.mean(episode_losses))
        
        # ì£¼ê¸°ì  ì„±ëŠ¥ ë° ê°€ì¤‘ì¹˜ ë³€í™” í™•ì¸
        if (episode + 1) % 50 == 0:
            current_weight = qat_agent.q_network.fc1.weight.data.clone()
            weight_change = torch.norm(current_weight - initial_weight).item()
            weight_changes.append(weight_change)
            
            recent_avg = np.mean(list(scores_window)[-50:])
            performance_checks.append(recent_avg)
            
            print(f"Episode {episode+1:3d}: "
                  f"Score={total_reward:3.0f}, "
                  f"Avg50={recent_avg:5.1f}, "
                  f"WeightÎ”={weight_change:.6f}, "
                  f"Îµ={qat_agent.epsilon:.3f}")
            
            # í•™ìŠµ ì •ì²´ í™•ì¸
            if len(weight_changes) >= 4:
                recent_changes = weight_changes[-4:]
                if max(recent_changes) - min(recent_changes) < 1e-6:
                    print("   âš ï¸  ê°€ì¤‘ì¹˜ ë³€í™”ê°€ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤. í•™ìŠµì´ ì •ì²´ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # 8. ìµœì¢… ì„±ëŠ¥ í‰ê°€
    print("\n8. ìµœì¢… ì„±ëŠ¥ í‰ê°€...")
    final_scores = evaluate_agent(qat_agent, env, episodes=100)
    final_avg = np.mean(final_scores)
    
    final_weight = qat_agent.q_network.fc1.weight.data.clone()
    total_weight_change = torch.norm(final_weight - initial_weight).item()
    
    # 9. ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ê°œì„ ëœ QAT ì „ì´ í•™ìŠµ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ”µ Original Normal DQN:     {normal_avg:.2f} Â± {np.std(normal_scores):.2f}")
    print(f"ğŸŸ¡ Initial QAT (ì›Œë°ì—…í›„):   {initial_avg:.2f} Â± {np.std(initial_scores):.2f}")
    print(f"ğŸ”´ Final QAT (íŒŒì¸íŠœë‹í›„):   {final_avg:.2f} Â± {np.std(final_scores):.2f}")
    print(f"ğŸ“ˆ ìµœì¢… ì„±ëŠ¥ ìœ ì§€ìœ¨:        {(final_avg/normal_avg)*100:.1f}%")
    print(f"ğŸ”§ ì´ ê°€ì¤‘ì¹˜ ë³€í™”ëŸ‰:        {total_weight_change:.6f}")
    print(f"ğŸ“š ìµœì¢… Buffer í¬ê¸°:        {len(qat_agent.memory)}")
    print(f"ğŸ¯ ìµœì¢… Epsilon:           {qat_agent.epsilon:.4f}")
    
    # í•™ìŠµ ì„±ê³µ ì—¬ë¶€ íŒë‹¨
    learning_success = (
        total_weight_change > 1e-4 and  # ê°€ì¤‘ì¹˜ê°€ ì¶©ë¶„íˆ ë³€í–ˆëŠ”ì§€
        final_avg > initial_avg * 0.9   # ì„±ëŠ¥ì´ ìœ ì§€ë˜ê±°ë‚˜ ê°œì„ ë˜ì—ˆëŠ”ì§€
    )
    
    if learning_success:
        print("âœ… QAT Fine-tuningì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸  QAT Fine-tuningì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print(f"   ê°€ì¤‘ì¹˜ ë³€í™”: {'ì¶©ë¶„í•¨' if total_weight_change > 1e-4 else 'ë¶€ì¡±í•¨'}")
        print(f"   ì„±ëŠ¥ ìœ ì§€: {'ì„±ê³µ' if final_avg > initial_avg * 0.9 else 'ì‹¤íŒ¨'}")
    
    return qat_agent

def plot_transfer_learning_results(normal_score, initial_qat_score, final_qat_score, training_scores):
    """ì „ì´ í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"""
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('QAT Transfer Learning Results', fontsize=16, fontweight='bold')
    
    # 1. ì„±ëŠ¥ ë¹„êµ ë°” ì°¨íŠ¸
    stages = ['Normal\n(Original)', 'QAT\n(Initial)', 'QAT\n(Fine-tuned)']
    scores = [normal_score, initial_qat_score, final_qat_score]
    colors = ['blue', 'orange', 'red']
    
    bars = ax1.bar(stages, scores, color=colors, alpha=0.7)
    ax1.set_title('Performance Comparison')
    ax1.set_ylabel('Average Score')
    ax1.grid(True, alpha=0.3)
    
    # ê°’ í‘œì‹œ
    for bar, score in zip(bars, scores):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,
                f'{score:.1f}', ha='center', va='bottom', fontweight='bold')
    
    # ì„±ëŠ¥ ìœ ì§€ìœ¨ í‘œì‹œ
    retention_rate = (final_qat_score / normal_score) * 100
    ax1.text(0.5, 0.95, f'Performance Retention: {retention_rate:.1f}%', 
             transform=ax1.transAxes, ha='center', va='top',
             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.8),
             fontsize=12, fontweight='bold')
    
    # 2. Fine-tuning í•™ìŠµ ê³¡ì„ 
    if training_scores:
        episodes = range(len(training_scores))
        ax2.plot(episodes, training_scores, color='red', alpha=0.7, label='QAT Fine-tuning')
        
        # ì´ë™í‰ê·  ì¶”ê°€
        if len(training_scores) >= 50:
            window_size = 50
            moving_avg = np.convolve(training_scores, np.ones(window_size)/window_size, mode='valid')
            ax2.plot(range(window_size-1, len(training_scores)), moving_avg, 
                    color='darkred', linewidth=2, label='Moving Average (50)')
        
        # ëª©í‘œì„  (Normal ì„±ëŠ¥)
        ax2.axhline(y=normal_score, color='blue', linestyle='--', alpha=0.8, 
                   label=f'Target (Normal: {normal_score:.1f})')
        
        ax2.set_title('QAT Fine-tuning Progress')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Score')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(RESULTS_DIR, 'qat_transfer_learning.png'), dpi=300, bbox_inches='tight')
    plt.show()
    try:
        checkpoint = torch.load(path, map_location='cpu')
        
        # ìƒˆë¡œìš´ í˜•ì‹ì˜ ì²´í¬í¬ì¸íŠ¸ì¸ì§€ í™•ì¸
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            is_quantized = checkpoint.get('is_quantized', False)
            
            if self.use_qat:
                if is_quantized:
                    # ì´ë¯¸ quantizedëœ ëª¨ë¸ ë¡œë“œ
                    print("Quantized ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
                    net_fp = QAT_DQN(self.state_size, self.action_size)
                    net_fp.fuse_model()
                    net_fp.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
                    torch.quantization.prepare_qat(net_fp, inplace=True)
                    net_q = torch.quantization.convert(net_fp.eval(), inplace=False)
                    net_q.load_state_dict(checkpoint['model_state_dict'])
                    self.q_network = net_q.to('cpu')
                else:
                    # QAT ëª¨ë¸ì´ì§€ë§Œ ì•„ì§ quantizedë˜ì§€ ì•Šì€ ê²½ìš°
                    print("QAT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
                    self.q_network = QAT_DQN(self.state_size, self.action_size)
                    self.q_network.fuse_model()
                    self.q_network.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
                    torch.quantization.prepare_qat(self.q_network, inplace=True)
                    self.q_network.load_state_dict(checkpoint['model_state_dict'])
                    self.q_network = self.q_network.to('cpu')
            else:
                # ì¼ë°˜ ëª¨ë¸
                self.q_network = DQN(self.state_size, self.action_size)
                self.q_network.load_state_dict(checkpoint['model_state_dict'])
                self.q_network = self.q_network.to(device)
            
            # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œë“œ
            self.training_scores = checkpoint.get('training_scores', [])
            self.training_losses = checkpoint.get('training_losses', [])
            self.epsilon_history = checkpoint.get('epsilon_history', [])
            
        else:
            # ê¸°ì¡´ í˜•ì‹ (state_dictë§Œ ì €ì¥ëœ ê²½ìš°)
            print(f"ê¸°ì¡´ í˜•ì‹ì˜ ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤: {path}")
            if self.use_qat:
                self.q_network = QAT_DQN(self.state_size, self.action_size)
                self.q_network.fuse_model()
                self.q_network.qconfig = torch.quantization.get_default_qat_qconfig('qnnpack')
                torch.quantization.prepare_qat(self.q_network, inplace=True)
                # ê¸°ì¡´ í˜•ì‹ì€ quantizedë˜ì§€ ì•Šì€ ìƒíƒœë¡œ ê°€ì •
                self.q_network.load_state_dict(checkpoint)
                self.q_network = self.q_network.to('cpu')
            else:
                self.q_network = DQN(self.state_size, self.action_size)
                self.q_network.load_state_dict(checkpoint)
                self.q_network = self.q_network.to(device)
            
            # ê¸°ì¡´ í˜•ì‹ì—ì„œëŠ” í›ˆë ¨ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŒ
            self.training_scores = []
            self.training_losses = []
            self.epsilon_history = []
            print("ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ì—ëŠ” í›ˆë ¨ íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        self.q_network.eval()
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ë„ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì„¤ì •
        if hasattr(self, 'target_network'):
            if self.use_qat:
                self.target_network = self.target_network.to('cpu')
            else:
                self.target_network = self.target_network.to(device)
        
        print(f"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {path}")
        print(f"ëª¨ë¸ ë””ë°”ì´ìŠ¤: {'CPU (QAT/Quantized)' if self.use_qat else device}")
        print(f"Quantized ìƒíƒœ: {checkpoint.get('is_quantized', 'Unknown')}")
        
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ëª¨ë¸ íŒŒì¼ì„ ë‹¤ì‹œ í›ˆë ¨í•´ì£¼ì„¸ìš”.")
        raise

def validate_qat_learning(agent, episodes=10):
    """QAT ëª¨ë¸ì˜ í•™ìŠµ ìƒíƒœ ê²€ì¦"""
    print("\n=== QAT í•™ìŠµ ìƒíƒœ ê²€ì¦ ===")
    
    # 1. ê°€ì¤‘ì¹˜ ë¶„ì„
    weight_stats = agent.analyze_weight_distribution()
    for layer_name, stats in weight_stats.items():
        print(f"{layer_name}: unique_values={stats['unique_values']}, std={stats['std']:.6f}")
    
    # 2. Gradient íë¦„ í™•ì¸
    if len(agent.memory) >= agent.batch_size:
        print("Gradient íë¦„ í…ŒìŠ¤íŠ¸...")
        for i in range(3):  # 3ë²ˆ í…ŒìŠ¤íŠ¸
            loss = agent.replay()
            if loss:
                print(f"  Test {i+1}: loss={loss:.6f}")
    
    # 3. ëª¨ë¸ ì¶œë ¥ ì¼ê´€ì„± í™•ì¸
    agent.q_network.eval()
    test_input = torch.randn(5, agent.state_size).to('cpu')
    
    outputs = []
    for _ in range(3):
        with torch.no_grad():
            output = agent.q_network(test_input)
            outputs.append(output)
    
    # ì¶œë ¥ ì¼ê´€ì„± í™•ì¸
    consistency = all(torch.allclose(outputs[0], out, atol=1e-6) for out in outputs[1:])
    print(f"ì¶œë ¥ ì¼ê´€ì„±: {'ì¼ê´€ë¨' if consistency else 'ë¶ˆì¼ì¹˜ (ì •ìƒì ì¸ quantization íš¨ê³¼)'}")
    
    agent.q_network.train()
    
    return {
        'weight_stats': weight_stats,
        'output_consistency': consistency
    }


# ì—ì´ì „íŠ¸ í•™ìŠµ í•¨ìˆ˜
def train_agent(agent, env, episodes=300, agent_name="Agent"):
    scores = []
    scores_window = deque(maxlen=100)
    
    for episode in range(episodes):
        obs = env.reset()
        if isinstance(obs, tuple):
            state = obs[0]
        else:
            state = obs
        total_reward = 0
        episode_losses = []
        
        while True:
            action = agent.act(state)
            step_result = env.step(action)
            
            if len(step_result) == 5:
                next_state, reward, terminated, truncated, info = step_result
                done = terminated or truncated
            else:
                next_state, reward, done, info = step_result
            
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward
            
            if done:
                break
            
            loss = agent.replay()
            if loss is not None:
                episode_losses.append(loss)
        
        print(f"Episode {episode} ê°œë³„ ì ìˆ˜: {total_reward}")

        scores_window.append(total_reward)
        scores.append(total_reward)
        agent.training_scores.append(total_reward)
        agent.epsilon_history.append(agent.epsilon)
        
        if episode_losses:
            agent.training_losses.append(np.mean(episode_losses))
        
        if episode % 10 == 0:  # ë” ìì£¼ ì¶œë ¥
            recent_scores = list(scores_window)[-10:]  # ìµœê·¼ 10ê°œ
            print(f"ìµœê·¼ 10ê°œ ì—í”¼ì†Œë“œ: {recent_scores}")
            print(f"í‰ê· : {np.mean(scores_window):.2f}")
    
    return scores

# ì—ì´ì „íŠ¸ í‰ê°€ í•¨ìˆ˜
def evaluate_agent(agent, env, episodes=100):
    scores = []
    original_epsilon = agent.epsilon
    agent.epsilon = 0
    
    for _ in range(episodes):
        obs = env.reset()
        state = obs[0] if isinstance(obs, tuple) else obs
        total_reward = 0
        done = False
        
        while not done:
            action = agent.act(state) # ì„ íƒëœ í–‰ë™ (0=ì™¼ìª½, 1=ì˜¤ë¥¸ìª½)
            step = env.step(action) # step : í™˜ê²½ì˜ ë°˜ì‘ (ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ ë“±)
            if len(step) == 5:
                next_state, reward, term, trunc, _ = step
                done = term or trunc
            else:
                next_state, reward, done, _ = step
            state = next_state
            total_reward += reward
        
        scores.append(total_reward)
    
    agent.epsilon = original_epsilon
    return scores

# ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ í•¨ìˆ˜ë“¤
def run_live_simulation(agent, env, agent_name="Agent", episodes=5, delay=0.05):
    """ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰"""
    print(f"\n=== {agent_name} ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ===")
    
    # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ì €ì¥ìš©
    simulation_data = {
        'positions': [],
        'angles': [],
        'actions': [],
        'rewards': [],
        'episode_scores': []
    }
    
    agent.epsilon = 0  # íƒí—˜ ë¹„í™œì„±í™”
    
    for episode in range(episodes):
        obs = env.reset()
        state = obs[0] if isinstance(obs, tuple) else obs
        total_reward = 0
        step_count = 0
        
        episode_positions = []
        episode_angles = []
        episode_actions = []
        episode_rewards = []
        
        print(f"\nì—í”¼ì†Œë“œ {episode + 1} ì‹œì‘...")
        
        while True:
            # í™˜ê²½ ë Œë”ë§
            if hasattr(env, 'render') and delay > 0:
                env.render()
            time.sleep(delay)
            
            # ìƒíƒœ ì •ë³´ ì €ì¥
            cart_pos = state[0]
            pole_angle = state[2]
            episode_positions.append(cart_pos)
            episode_angles.append(pole_angle)
            
            # í–‰ë™ ì„ íƒ
            action = agent.act(state)
            episode_actions.append(action)
            
            # í™˜ê²½ ìŠ¤í…
            step_result = env.step(action)
            if len(step_result) == 5:  # ìµœì‹  gym (terminated, truncated ë¶„ë¦¬)
                next_state, reward, terminated, truncated, info = step_result
                done = terminated or truncated
            else:  # êµ¬ë²„ì „ gym
                next_state, reward, done, info = step_result
            
            episode_rewards.append(reward)
            state = next_state
            total_reward += reward
            step_count += 1
            
            if done:
                break
        
        # ì—í”¼ì†Œë“œ ë°ì´í„° ì €ì¥
        simulation_data['positions'].append(episode_positions)
        simulation_data['angles'].append(episode_angles)
        simulation_data['actions'].append(episode_actions)
        simulation_data['rewards'].append(episode_rewards)
        simulation_data['episode_scores'].append(total_reward)
        
        print(f"ì—í”¼ì†Œë“œ {episode + 1} ì™„ë£Œ - ì ìˆ˜: {total_reward}, ìŠ¤í…: {step_count}")
    
    print(f"\n{agent_name} í‰ê·  ì ìˆ˜: {np.mean(simulation_data['episode_scores']):.2f}")
    return simulation_data

def compare_live_simulations(normal_agent, qat_agent, env, episodes=3):
    """ë‘ ì—ì´ì „íŠ¸ì˜ ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ë¹„êµ"""
    print("\n=== ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ë¹„êµ ===")
    
    # Normal ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜
    print("\n1. Normal DQN ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì¤‘...")
    normal_data = run_live_simulation(normal_agent, env, "Normal DQN", episodes)
    
    input("\nQAT DQN ì‹œë®¬ë ˆì´ì…˜ì„ ì‹œì‘í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    
    # QAT ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜
    print("\n2. QAT DQN ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ì¤‘...")
    qat_data = run_live_simulation(qat_agent, env, "QAT DQN", episodes)
    
    # ê²°ê³¼ ë¹„êµ ì‹œê°í™”
    plot_simulation_comparison(normal_data, qat_data)
    
    return normal_data, qat_data

def plot_simulation_comparison(normal_data, qat_data):
    """ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ë¹„êµ ì‹œê°í™”"""
    # results ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Live Simulation Comparison', fontsize=16, fontweight='bold')
    
    # 1. í‰ê·  ì„±ëŠ¥ ë¹„êµ
    normal_scores = normal_data['episode_scores']
    qat_scores = qat_data['episode_scores']
    
    episodes = range(1, len(normal_scores) + 1)
    width = 0.35
    x = np.arange(len(episodes))
    
    axes[0,0].bar(x - width/2, normal_scores, width, label='Normal DQN', color='blue', alpha=0.7)
    axes[0,0].bar(x + width/2, qat_scores, width, label='QAT DQN', color='red', alpha=0.7)
    axes[0,0].set_title('Episode Scores Comparison')
    axes[0,0].set_xlabel('Episode')
    axes[0,0].set_ylabel('Score')
    axes[0,0].set_xticks(x)
    axes[0,0].set_xticklabels([f'Ep{i}' for i in episodes])
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. ì¹´íŠ¸ ìœ„ì¹˜ ë³€í™” (ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œ)
    if normal_data['positions'] and qat_data['positions']:
        normal_pos = normal_data['positions'][0]
        qat_pos = qat_data['positions'][0]
        
        steps_normal = range(len(normal_pos))
        steps_qat = range(len(qat_pos))
        
        axes[0,1].plot(steps_normal, normal_pos, label='Normal DQN', color='blue', alpha=0.7)
        axes[0,1].plot(steps_qat, qat_pos, label='QAT DQN', color='red', alpha=0.7)
        axes[0,1].set_title('Cart Position Over Time (Episode 1)')
        axes[0,1].set_xlabel('Step')
        axes[0,1].set_ylabel('Cart Position')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        axes[0,1].axhline(y=2.4, color='orange', linestyle='--', alpha=0.5, label='Boundary')
        axes[0,1].axhline(y=-2.4, color='orange', linestyle='--', alpha=0.5)
    
    # 3. í´ ê°ë„ ë³€í™” (ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œ)
    if normal_data['angles'] and qat_data['angles']:
        normal_angles = normal_data['angles'][0]
        qat_angles = qat_data['angles'][0]
        
        steps_normal = range(len(normal_angles))
        steps_qat = range(len(qat_angles))
        
        axes[1,0].plot(steps_normal, np.degrees(normal_angles), label='Normal DQN', color='blue', alpha=0.7)
        axes[1,0].plot(steps_qat, np.degrees(qat_angles), label='QAT DQN', color='red', alpha=0.7)
        axes[1,0].set_title('Pole Angle Over Time (Episode 1)')
        axes[1,0].set_xlabel('Step')
        axes[1,0].set_ylabel('Pole Angle (degrees)')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
        axes[1,0].axhline(y=12, color='orange', linestyle='--', alpha=0.5, label='Boundary')
        axes[1,0].axhline(y=-12, color='orange', linestyle='--', alpha=0.5)
    
    # 4. í–‰ë™ ë¶„í¬ ë¹„êµ
    all_normal_actions = []
    all_qat_actions = []
    
    for episode_actions in normal_data['actions']:
        all_normal_actions.extend(episode_actions)
    for episode_actions in qat_data['actions']:
        all_qat_actions.extend(episode_actions)
    
    action_labels = ['Left', 'Right']
    normal_action_counts = [all_normal_actions.count(0), all_normal_actions.count(1)]
    qat_action_counts = [all_qat_actions.count(0), all_qat_actions.count(1)]
    
    x = np.arange(len(action_labels))
    width = 0.35
    
    axes[1,1].bar(x - width/2, normal_action_counts, width, label='Normal DQN', color='blue', alpha=0.7)
    axes[1,1].bar(x + width/2, qat_action_counts, width, label='QAT DQN', color='red', alpha=0.7)
    axes[1,1].set_title('Action Distribution')
    axes[1,1].set_xlabel('Action')
    axes[1,1].set_ylabel('Count')
    axes[1,1].set_xticks(x)
    axes[1,1].set_xticklabels(action_labels)
    axes[1,1].legend()
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(RESULTS_DIR, 'live_simulation_comparison.png'), dpi=300, bbox_inches='tight')
    plt.show()

def create_animated_comparison(normal_data, qat_data):
    """ì• ë‹ˆë©”ì´ì…˜ ë¹„êµ ìƒì„±"""
    import matplotlib.animation as animation
    
    # results ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(RESULTS_DIR, exist_ok=True)
    
    if not normal_data['positions'] or not qat_data['positions']:
        print("ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œ ë°ì´í„° ì‚¬ìš©
    normal_pos = normal_data['positions'][0]
    normal_angles = normal_data['angles'][0]
    qat_pos = qat_data['positions'][0]
    qat_angles = qat_data['angles'][0]
    
    # ë” ê¸´ ì—í”¼ì†Œë“œì— ë§ì¶° ì¡°ì •
    max_steps = max(len(normal_pos), len(qat_pos))
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    fig.suptitle('Real-time CartPole Comparison Animation', fontsize=16, fontweight='bold')
    
    # Normal DQN ì„œë¸Œí”Œë¡¯
    ax1.set_xlim(-3, 3)
    ax1.set_ylim(-0.5, 2)
    ax1.set_title('Normal DQN')
    ax1.set_xlabel('Position')
    ax1.grid(True, alpha=0.3)
    
    # QAT DQN ì„œë¸Œí”Œë¡¯
    ax2.set_xlim(-3, 3)
    ax2.set_ylim(-0.5, 2)
    ax2.set_title('QAT DQN')
    ax2.set_xlabel('Position')
    ax2.grid(True, alpha=0.3)
    
    # ê·¸ë˜í”½ ìš”ì†Œ ì´ˆê¸°í™”
    from matplotlib.patches import Rectangle
    cart1 = Rectangle((-0.25, 0), 0.5, 0.3, fc='blue', alpha=0.7)
    pole1, = ax1.plot([], [], 'b-', linewidth=8)
    ax1.add_patch(cart1)
    
    cart2 = Rectangle((-0.25, 0), 0.5, 0.3, fc='red', alpha=0.7)
    pole2, = ax2.plot([], [], 'r-', linewidth=8)
    ax2.add_patch(cart2)
    
    # ì ìˆ˜ í…ìŠ¤íŠ¸
    score_text1 = ax1.text(0.02, 0.95, '', transform=ax1.transAxes, fontsize=12,
                          bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    score_text2 = ax2.text(0.02, 0.95, '', transform=ax2.transAxes, fontsize=12,
                          bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    def animate(frame):
        # Normal DQN ì—…ë°ì´íŠ¸
        if frame < len(normal_pos):
            cart_x1 = normal_pos[frame]
            pole_angle1 = normal_angles[frame]
            
            # ì¹´íŠ¸ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
            cart1.set_x(cart_x1 - 0.25)
            
            # í´ ìœ„ì¹˜ ê³„ì‚° (ê¸¸ì´ 1.0 ê°€ì •)
            pole_x1 = [cart_x1, cart_x1 + np.sin(pole_angle1)]
            pole_y1 = [0.15, 0.15 + np.cos(pole_angle1)]
            pole1.set_data(pole_x1, pole_y1)
            
            score_text1.set_text(f'Step: {frame + 1}\nScore: {frame + 1}')
        
        # QAT DQN ì—…ë°ì´íŠ¸
        if frame < len(qat_pos):
            cart_x2 = qat_pos[frame]
            pole_angle2 = qat_angles[frame]
            
            # ì¹´íŠ¸ ìœ„ì¹˜ ì—…ë°ì´íŠ¸
            cart2.set_x(cart_x2 - 0.25)
            
            # í´ ìœ„ì¹˜ ê³„ì‚°
            pole_x2 = [cart_x2, cart_x2 + np.sin(pole_angle2)]
            pole_y2 = [0.15, 0.15 + np.cos(pole_angle2)]
            pole2.set_data(pole_x2, pole_y2)
            
            score_text2.set_text(f'Step: {frame + 1}\nScore: {frame + 1}')
        
        return cart1, pole1, cart2, pole2, score_text1, score_text2
    
    # ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
    anim = animation.FuncAnimation(fig, animate, frames=max_steps, 
                                 interval=100, blit=True, repeat=True)
    
    plt.tight_layout()
    
    # GIFë¡œ ì €ì¥ (ì„ íƒì‚¬í•­)
    try:
        anim.save(os.path.join(RESULTS_DIR, 'cartpole_comparison.gif'), 
                 writer='pillow', fps=10)
        print(f"ì• ë‹ˆë©”ì´ì…˜ì´ '{RESULTS_DIR}/cartpole_comparison.gif'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except:
        print("GIF ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. pillow íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    plt.show()
    return anim

# ì‹œê°í™” í•¨ìˆ˜ë“¤
class ModelComparator:
    def __init__(self, normal_agent, qat_agent):
        self.normal_agent = normal_agent
        self.qat_agent = qat_agent
        os.makedirs(RESULTS_DIR, exist_ok=True)
    
    def plot_training_comparison(self):
        """í›ˆë ¨ ê³¼ì • ë¹„êµ ì‹œê°í™”"""
        # results ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(RESULTS_DIR, exist_ok=True)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Training Progress Comparison: Normal DQN vs QAT DQN', fontsize=16, fontweight='bold')
        
        # 1. ìŠ¤ì½”ì–´ ë¹„êµ
        if self.normal_agent.training_scores and self.qat_agent.training_scores:
            episodes_normal = range(len(self.normal_agent.training_scores))
            episodes_qat = range(len(self.qat_agent.training_scores))
            
            axes[0,0].plot(episodes_normal, self.normal_agent.training_scores, 
                          label='Normal DQN', alpha=0.7, color='blue')
            axes[0,0].plot(episodes_qat, self.qat_agent.training_scores, 
                          label='QAT DQN', alpha=0.7, color='red')
            
            # ì´ë™í‰ê·  ì¶”ê°€
            window_size = 50
            if len(self.normal_agent.training_scores) >= window_size:
                normal_ma = np.convolve(self.normal_agent.training_scores, 
                                      np.ones(window_size)/window_size, mode='valid')
                axes[0,0].plot(range(window_size-1, len(self.normal_agent.training_scores)), 
                              normal_ma, label='Normal MA', color='darkblue', linewidth=2)
            
            if len(self.qat_agent.training_scores) >= window_size:
                qat_ma = np.convolve(self.qat_agent.training_scores, 
                                   np.ones(window_size)/window_size, mode='valid')
                axes[0,0].plot(range(window_size-1, len(self.qat_agent.training_scores)), 
                              qat_ma, label='QAT MA', color='darkred', linewidth=2)
            
            axes[0,0].set_title('Training Scores')
            axes[0,0].set_xlabel('Episode')
            axes[0,0].set_ylabel('Score')
            axes[0,0].legend()
            axes[0,0].grid(True, alpha=0.3)
        
        # 2. ì†ì‹¤ í•¨ìˆ˜ ë¹„êµ
        if self.normal_agent.training_losses and self.qat_agent.training_losses:
            axes[0,1].plot(self.normal_agent.training_losses, 
                          label='Normal DQN', alpha=0.7, color='blue')
            axes[0,1].plot(self.qat_agent.training_losses, 
                          label='QAT DQN', alpha=0.7, color='red')
            axes[0,1].set_title('Training Loss')
            axes[0,1].set_xlabel('Episode')
            axes[0,1].set_ylabel('Loss')
            axes[0,1].legend()
            axes[0,1].grid(True, alpha=0.3)
            axes[0,1].set_yscale('log')
        
        # 3. Epsilon ê°ì†Œ ë¹„êµ
        if self.normal_agent.epsilon_history and self.qat_agent.epsilon_history:
            axes[1,0].plot(self.normal_agent.epsilon_history, 
                          label='Normal DQN', color='blue')
            axes[1,0].plot(self.qat_agent.epsilon_history, 
                          label='QAT DQN', color='red')
            axes[1,0].set_title('Epsilon Decay')
            axes[1,0].set_xlabel('Episode')
            axes[1,0].set_ylabel('Epsilon')
            axes[1,0].legend()
            axes[1,0].grid(True, alpha=0.3)
        
        # 4. ìµœì¢… ì„±ëŠ¥ í†µê³„
        normal_final_scores = self.normal_agent.training_scores[-100:] if len(self.normal_agent.training_scores) >= 100 else self.normal_agent.training_scores
        qat_final_scores = self.qat_agent.training_scores[-100:] if len(self.qat_agent.training_scores) >= 100 else self.qat_agent.training_scores
        
        if normal_final_scores and qat_final_scores:
            performance_data = [normal_final_scores, qat_final_scores]
            labels = ['Normal DQN', 'QAT DQN']
            
            box_plot = axes[1,1].boxplot(performance_data, labels=labels, patch_artist=True)
            box_plot['boxes'][0].set_facecolor('lightblue')
            box_plot['boxes'][1].set_facecolor('lightcoral')
            axes[1,1].set_title('Final Performance Distribution\n(Last 100 episodes)')
            axes[1,1].set_ylabel('Score')
            axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(RESULTS_DIR, 'training_comparison.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_weight_distribution_comparison(self):
        """ê°€ì¤‘ì¹˜ ë¶„í¬ ë¹„êµ"""
        # results ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(RESULTS_DIR, exist_ok=True)
        
        normal_weights = self.normal_agent.analyze_weight_distribution()
        qat_weights = self.qat_agent.analyze_weight_distribution()
        
        # ê³µí†µ ë ˆì´ì–´ ì°¾ê¸°
        common_layers = set(normal_weights.keys()) & set(qat_weights.keys())
        
        if not common_layers:
            print("ê³µí†µ ë ˆì´ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        n_layers = len(common_layers)
        fig, axes = plt.subplots(n_layers, 2, figsize=(15, 4*n_layers))
        fig.suptitle('Weight Distribution Comparison', fontsize=16, fontweight='bold')
        
        if n_layers == 1:
            axes = axes.reshape(1, -1)
        
        for i, layer_name in enumerate(sorted(common_layers)):
            # Normal ëª¨ë¸ íˆìŠ¤í† ê·¸ë¨
            normal_data = normal_weights[layer_name]['data']
            axes[i,0].hist(normal_data, bins=50, alpha=0.7, color='blue', density=True)
            axes[i,0].set_title(f'Normal DQN - {layer_name}')
            axes[i,0].set_xlabel('Weight Value')
            axes[i,0].set_ylabel('Density')
            axes[i,0].grid(True, alpha=0.3)
            
            # í†µê³„ ì •ë³´ ì¶”ê°€
            stats_text = f'Mean: {normal_weights[layer_name]["mean"]:.4f}\n' \
                        f'Std: {normal_weights[layer_name]["std"]:.4f}\n' \
                        f'Unique: {normal_weights[layer_name]["unique_values"]}'
            axes[i,0].text(0.02, 0.98, stats_text, transform=axes[i,0].transAxes, 
                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
            
            # QAT ëª¨ë¸ íˆìŠ¤í† ê·¸ë¨
            qat_data = qat_weights[layer_name]['data']
            axes[i,1].hist(qat_data, bins=50, alpha=0.7, color='red', density=True)
            axes[i,1].set_title(f'QAT DQN - {layer_name}')
            axes[i,1].set_xlabel('Weight Value')
            axes[i,1].set_ylabel('Density')
            axes[i,1].grid(True, alpha=0.3)
            
            # í†µê³„ ì •ë³´ ì¶”ê°€
            stats_text = f'Mean: {qat_weights[layer_name]["mean"]:.4f}\n' \
                        f'Std: {qat_weights[layer_name]["std"]:.4f}\n' \
                        f'Unique: {qat_weights[layer_name]["unique_values"]}'
            axes[i,1].text(0.02, 0.98, stats_text, transform=axes[i,1].transAxes, 
                          verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(os.path.join(RESULTS_DIR, 'weight_distribution_comparison.png'), dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_performance_metrics(self, eval_env):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ"""
        print("ì„±ëŠ¥ í‰ê°€ ì¤‘...")
        
        # results ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(RESULTS_DIR, exist_ok=True)
        
        # í‰ê°€ ìˆ˜í–‰
        normal_eval_scores = evaluate_agent(self.normal_agent, eval_env, episodes=100)
        qat_eval_scores = evaluate_agent(self.qat_agent, eval_env, episodes=100)
        
        # ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
        normal_speed = self.normal_agent.benchmark_inference_speed(1000)
        qat_speed = self.qat_agent.benchmark_inference_speed(1000)
        
        # ëª¨ë¸ í¬ê¸°
        normal_size = self.normal_agent.get_model_size_mb()
        qat_size = self.qat_agent.get_model_size_mb()
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Performance Metrics Comparison', fontsize=16, fontweight='bold')
        
        # 1. í‰ê°€ ì ìˆ˜ ë¶„í¬
        axes[0,0].boxplot([normal_eval_scores, qat_eval_scores], 
                         labels=['Normal DQN', 'QAT DQN'], patch_artist=True)
        axes[0,0].set_title('Evaluation Scores Distribution')
        axes[0,0].set_ylabel('Score')
        axes[0,0].grid(True, alpha=0.3)
        
        # 2. ì¶”ë¡  ì†ë„ ë¹„êµ
        models = ['Normal DQN', 'QAT DQN']
        throughputs = [normal_speed['throughput'], qat_speed['throughput']]
        colors = ['blue', 'red']
        
        bars = axes[0,1].bar(models, throughputs, color=colors, alpha=0.7)
        axes[0,1].set_title('Inference Throughput')
        axes[0,1].set_ylabel('Samples/sec')
        axes[0,1].grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, throughputs):
            axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,
                          f'{value:.1f}', ha='center', va='bottom')
        
        # 3. ëª¨ë¸ í¬ê¸° ë¹„êµ
        sizes = [normal_size, qat_size]
        bars = axes[1,0].bar(models, sizes, color=colors, alpha=0.7)
        axes[1,0].set_title('Model Size')
        axes[1,0].set_ylabel('Size (MB)')
        axes[1,0].grid(True, alpha=0.3)
        
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, sizes):
            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                          f'{value:.3f}', ha='center', va='bottom')
        
        # 4. ì¢…í•© ë©”íŠ¸ë¦­ ë¹„êµ
        categories = ['Performance\n(Score)', 'Speed\n(Throughput)', 'Efficiency\n(1/Size)']
        
        # ì •ê·œí™” (0-1 ìŠ¤ì¼€ì¼)
        normal_perf = np.mean(normal_eval_scores) / 500  # CartPole ìµœëŒ€ ì ìˆ˜ ê¸°ì¤€
        qat_perf = np.mean(qat_eval_scores) / 500
        
        normal_speed_norm = normal_speed['throughput'] / max(normal_speed['throughput'], qat_speed['throughput'])
        qat_speed_norm = qat_speed['throughput'] / max(normal_speed['throughput'], qat_speed['throughput'])
        
        normal_eff = (1/normal_size) / max(1/normal_size, 1/qat_size)
        qat_eff = (1/qat_size) / max(1/normal_size, 1/qat_size)
        
        normal_values = [normal_perf, normal_speed_norm, normal_eff]
        qat_values = [qat_perf, qat_speed_norm, qat_eff]
        
        x = np.arange(len(categories))
        width = 0.35
        
        axes[1,1].bar(x - width/2, normal_values, width, label='Normal DQN', color='blue', alpha=0.7)
        axes[1,1].bar(x + width/2, qat_values, width, label='QAT DQN', color='red', alpha=0.7)
        
        axes[1,1].set_title('Normalized Metrics Comparison')
        axes[1,1].set_ylabel('Normalized Score (0-1)')
        axes[1,1].set_xticks(x)
        axes[1,1].set_xticklabels(categories)
        axes[1,1].legend()
        axes[1,1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(RESULTS_DIR, 'performance_metrics.png'), dpi=300, bbox_inches='tight')
        plt.show()
        
        # ìˆ˜ì¹˜ ê²°ê³¼ ì¶œë ¥
        print("\n=== ì„±ëŠ¥ ë¹„êµ ê²°ê³¼ ===")
        print(f"Normal DQN - í‰ê·  ì ìˆ˜: {np.mean(normal_eval_scores):.2f} Â± {np.std(normal_eval_scores):.2f}")
        print(f"QAT DQN - í‰ê·  ì ìˆ˜: {np.mean(qat_eval_scores):.2f} Â± {np.std(qat_eval_scores):.2f}")
        print(f"Normal DQN - ì¶”ë¡  ì†ë„: {normal_speed['throughput']:.1f} samples/sec")
        print(f"QAT DQN - ì¶”ë¡  ì†ë„: {qat_speed['throughput']:.1f} samples/sec")
        print(f"Normal DQN - ëª¨ë¸ í¬ê¸°: {normal_size:.3f} MB")
        print(f"QAT DQN - ëª¨ë¸ í¬ê¸°: {qat_size:.3f} MB")
        print(f"í¬ê¸° ì••ì¶•ë¥ : {(normal_size/qat_size):.2f}x")
    
    def generate_comprehensive_report(self, eval_env):
        """ì¢…í•© ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("ì¢…í•© ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ëª¨ë“  ì‹œê°í™” ìƒì„±
        self.plot_training_comparison()
        self.plot_weight_distribution_comparison()
        self.plot_performance_metrics(eval_env)
        
        print(f"\nëª¨ë“  ë¹„êµ ì°¨íŠ¸ê°€ '{RESULTS_DIR}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("- training_comparison.png: í›ˆë ¨ ê³¼ì • ë¹„êµ")
        print("- weight_distribution_comparison.png: ê°€ì¤‘ì¹˜ ë¶„í¬ ë¹„êµ")
        print("- performance_metrics.png: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ")

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def print_model_info(agent, agent_name):
    """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
    print(f"\n=== {agent_name} ëª¨ë¸ ì •ë³´ ===")
    print(f"ëª¨ë¸ íƒ€ì…: {'QAT' if agent.use_qat else 'Normal'}")
    print(f"ëª¨ë¸ í¬ê¸°: {agent.get_model_size_mb():.3f} MB")
    
    # ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
    print("ëª¨ë¸ êµ¬ì¡°:")
    for name, module in agent.q_network.named_modules():
        if len(list(module.children())) == 0:  # leaf ëª¨ë“ˆë§Œ
            print(f"  {name}: {module}")
    
    # ê°€ì¤‘ì¹˜ í†µê³„
    weight_stats = agent.analyze_weight_distribution()
    print("ê°€ì¤‘ì¹˜ í†µê³„:")
    for layer_name, stats in weight_stats.items():
        print(f"  {layer_name}: "
              f"min={stats['min']:.4f}, max={stats['max']:.4f}, "
              f"mean={stats['mean']:.4f}, std={stats['std']:.4f}, "
              f"unique_values={stats['unique_values']}")

def print_usage_examples():
    """ì‚¬ìš© ì˜ˆì‹œ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸš€ QAT vs Normal DQN ë¹„êµ ë¶„ì„ ë„êµ¬")
    print("="*60)
    print()
    print("1. ì²˜ìŒ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:")
    print("   python test2.py --mode train_normal")
    print("   python test2.py --mode transfer_qat --episodes 1000")
    print("   python test2.py --mode compare")
    print()
    print("2. ğŸŒŸ QAT ì „ì´ í•™ìŠµ (ì¶”ì²œ):")
    print("   python test2.py --mode transfer_qat --episodes 1000")
    print()
    print("3. ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ë³´ê¸°:")
    print("   python test2.py --mode simulate")
    print()
    print("4. ì• ë‹ˆë©”ì´ì…˜ ìƒì„±:")
    print("   python test2.py --mode animate")
    print()
    print("5. ê¸°ì¡´ ë°©ì‹ (ì°¸ê³ ìš©):")
    print("   python test2.py --mode train_both")
    print()
    print("ğŸ“ ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜:")
    print(f"   ëª¨ë¸: {MODEL_DIR}/")
    print(f"   ì‹œê°í™”: {RESULTS_DIR}/")
    print()
    print("ğŸ’¡ ê¶Œì¥ ìˆœì„œ:")
    print("   1) train_normal (2000+ episodes)")
    print("   2) transfer_qat (1000 episodes)")  
    print("   3) compare (ì „ì²´ ë¶„ì„)")
    print("="*60)

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == '__main__':
    # ì¸ìˆ˜ê°€ ì—†ìœ¼ë©´ ë„ì›€ë§ ì¶œë ¥
    if len(sys.argv) == 1:
        print_usage_examples()
        parser = argparse.ArgumentParser(description='QAT vs Normal DQN ë¹„êµ ë¶„ì„ ë„êµ¬')
        parser.add_argument('--mode', choices=['train_normal', 'train_qat', 'train_both', 'compare', 'simulate', 'animate', 'transfer_qat'], 
                          required=True, help='ì‹¤í–‰ ëª¨ë“œ ì„ íƒ')
        parser.print_help()
        sys.exit(0)
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', choices=['train_normal', 'train_qat', 'train_both', 'compare', 'simulate', 'animate', 'transfer_qat'], required=True)
    args = parser.parse_args()

    # í™˜ê²½ ìƒì„±
    env = gym.make('CartPole-v1')
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    
    print(f"State size: {state_size}, Action size: {action_size}")
    
    if args.mode == 'train_normal':
        print("\n=== ì¼ë°˜ DQN í›ˆë ¨ ===")
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        train_agent(normal_agent, env, episodes=TRAIN_EPISODES, agent_name="Normal DQN")
        normal_agent.save(NORMAL_MODEL_PATH)
        print(f"Normal DQN ëª¨ë¸ì´ {NORMAL_MODEL_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    elif args.mode == 'train_qat':
        print("\n=== QAT DQN í›ˆë ¨ ===")
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        train_agent(qat_agent, env, episodes=TRAIN_EPISODES, agent_name="QAT DQN")
        qat_agent.save(QAT_MODEL_PATH)
        print(f"QAT DQN ëª¨ë¸ì´ {QAT_MODEL_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    elif args.mode == 'transfer_qat':
        print("\n=== QAT ì „ì´ í•™ìŠµ ===")
        
        # ì‚¬ì „ í›ˆë ¨ëœ Normal ëª¨ë¸ í™•ì¸
        if not os.path.exists(NORMAL_MODEL_PATH):
            print("âŒ ì‚¬ì „ í›ˆë ¨ëœ Normal ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ Normal ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”:")
            print("python test2.py --mode train_normal")
            sys.exit(1)
        
        # QAT ì „ì´ í•™ìŠµ ì‹¤í–‰
        qat_agent = improved_transfer_learn_qat(
            NORMAL_MODEL_PATH, state_size, action_size, env, episodes=2000
        )
        
        if qat_agent:
            # í•™ìŠµ ê²€ì¦
            validate_qat_learning(qat_agent)
            
            # ëª¨ë¸ ì €ì¥
            qat_agent.save(QAT_MODEL_PATH)
            print(f"\nâœ… ê°œì„ ëœ QAT ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {QAT_MODEL_PATH}")
        
    elif args.mode == 'train_both':
        print("\n=== ë‘ ëª¨ë¸ ëª¨ë‘ í›ˆë ¨ ===")
        
        # Normal DQN í›ˆë ¨
        print("\n1. ì¼ë°˜ DQN í›ˆë ¨ ì‹œì‘...")
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        train_agent(normal_agent, env, episodes=TRAIN_EPISODES, agent_name="Normal DQN")
        normal_agent.save(NORMAL_MODEL_PATH)
        
        # QAT DQN í›ˆë ¨
        print("\n2. QAT DQN í›ˆë ¨ ì‹œì‘...")
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        train_agent(qat_agent, env, episodes=TRAIN_EPISODES, agent_name="QAT DQN")
        qat_agent.save(QAT_MODEL_PATH)
        
        print("\në‘ ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    elif args.mode == 'compare':
        print("\n=== ëª¨ë¸ ë¹„êµ ë¶„ì„ ===")
        
        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(NORMAL_MODEL_PATH) or not os.path.exists(QAT_MODEL_PATH):
            print("í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”:")
            print("  python test2.py --mode train_both")
            print("  python test2.py --mode transfer_qat")
            sys.exit(1)
        
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        normal_agent.load(NORMAL_MODEL_PATH)
        
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        qat_agent.load(QAT_MODEL_PATH)
        
        # ë¹„êµ ë¶„ì„ ì‹¤í–‰
        comparator = ModelComparator(normal_agent, qat_agent)
        comparator.generate_comprehensive_report(env)
        
    elif args.mode == 'simulate':
        print("\n=== ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ===")
        
        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(NORMAL_MODEL_PATH) or not os.path.exists(QAT_MODEL_PATH):
            print("í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”:")
            print("  python test2.py --mode transfer_qat")
            sys.exit(1)
        
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        normal_agent.load(NORMAL_MODEL_PATH)
        
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        qat_agent.load(QAT_MODEL_PATH)
        
        # ë Œë”ë§ ëª¨ë“œë¡œ í™˜ê²½ ì¬ìƒì„±
        env.close()
        env = gym.make('CartPole-v1', render_mode='human')
        
        # ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        normal_data, qat_data = compare_live_simulations(normal_agent, qat_agent, env, episodes=3)
        
        print("\nì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ê²°ê³¼ íŒŒì¼: {RESULTS_DIR}/live_simulation_comparison.png")
        
    elif args.mode == 'animate':
        print("\n=== ì• ë‹ˆë©”ì´ì…˜ ë¹„êµ ëª¨ë“œ ===")
        
        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(NORMAL_MODEL_PATH) or not os.path.exists(QAT_MODEL_PATH):
            print("í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ë¨¼ì € ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”:")
            print("  python test2.py --mode transfer_qat")
            sys.exit(1)
        
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        normal_agent.load(NORMAL_MODEL_PATH)
        
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        qat_agent.load(QAT_MODEL_PATH)
        
        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± (ë Œë”ë§ ì—†ì´)
        print("ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì¤‘...")
        normal_data = run_live_simulation(normal_agent, env, "Normal DQN", episodes=1, delay=0)
        qat_data = run_live_simulation(qat_agent, env, "QAT DQN", episodes=1, delay=0)
        
        # ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        print("ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì¤‘...")
        anim = create_animated_comparison(normal_data, qat_data)
        
        print("ì• ë‹ˆë©”ì´ì…˜ ë¹„êµê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ê²°ê³¼ íŒŒì¼: {RESULTS_DIR}/cartpole_comparison.gif")
    
    env.close()
    print("\ní”„ë¡œê·¸ë¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ëª¨ë“  ê²°ê³¼ íŒŒì¼ì€ '{RESULTS_DIR}/' í´ë”ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ë‹¤ìŒ ë‹¨ê³„ ì•ˆë‚´
    if args.mode == 'transfer_qat':
        print("\nğŸ‰ QAT ì „ì´ í•™ìŠµ ì™„ë£Œ!")
        print("ë‹¤ìŒ ë‹¨ê³„:")
        print("  python test2.py --mode compare     # ìƒì„¸ ë¶„ì„")
        print("  python test2.py --mode simulate    # ì‹¤ì‹œê°„ ë¹„êµ")
    elif args.mode == 'train_normal':
        print("\nâœ… Normal DQN í›ˆë ¨ ì™„ë£Œ!")
        print("ë‹¤ìŒ ë‹¨ê³„:")
        print("  python test2.py --mode transfer_qat --episodes 1000")
    elif args.mode == 'compare':
        print("\nğŸ“Š ë¶„ì„ ì™„ë£Œ! results/ í´ë”ì˜ ì°¨íŠ¸ë“¤ì„ í™•ì¸í•´ë³´ì„¸ìš”.")

# ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def compare_model_sizes():
    """ëª¨ë¸ í¬ê¸° ë¹„êµ ìœ í‹¸ë¦¬í‹°"""
    if os.path.exists(NORMAL_MODEL_PATH) and os.path.exists(QAT_MODEL_PATH):
        normal_size = os.path.getsize(NORMAL_MODEL_PATH) / (1024 * 1024)
        qat_size = os.path.getsize(QAT_MODEL_PATH) / (1024 * 1024)
        
        print(f"\nğŸ“Š ëª¨ë¸ íŒŒì¼ í¬ê¸° ë¹„êµ:")
        print(f"Normal DQN: {normal_size:.3f} MB")
        print(f"QAT DQN:    {qat_size:.3f} MB")
        print(f"ì••ì¶•ë¥ :     {normal_size/qat_size:.2f}x")
        
        return normal_size, qat_size
    else:
        print("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

def quick_performance_test():
    """ë¹ ë¥¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    if not (os.path.exists(NORMAL_MODEL_PATH) and os.path.exists(QAT_MODEL_PATH)):
        print("ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    env = gym.make('CartPole-v1')
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n
    
    # ëª¨ë¸ ë¡œë“œ
    normal_agent = DQNAgent(state_size, action_size, use_qat=False)
    normal_agent.load(NORMAL_MODEL_PATH)
    
    qat_agent = DQNAgent(state_size, action_size, use_qat=True)
    qat_agent.load(QAT_MODEL_PATH)
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (20 ì—í”¼ì†Œë“œ)
    print("ë¹ ë¥¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
    normal_scores = evaluate_agent(normal_agent, env, episodes=20)
    qat_scores = evaluate_agent(qat_agent, env, episodes=20)
    
    print(f"\nğŸ“Š ë¹ ë¥¸ ì„±ëŠ¥ ë¹„êµ (20 ì—í”¼ì†Œë“œ):")
    print(f"Normal DQN: {np.mean(normal_scores):.1f} Â± {np.std(normal_scores):.1f}")
    print(f"QAT DQN:    {np.mean(qat_scores):.1f} Â± {np.std(qat_scores):.1f}")
    print(f"ì„±ëŠ¥ ìœ ì§€ìœ¨: {(np.mean(qat_scores)/np.mean(normal_scores)*100):.1f}%")
    
    env.close()
    
    if __name__ == '__main__' and len(sys.argv) > 1 and sys.argv[1] == '--quick-test':
        quick_performance_test()
    elif __name__ == '__main__' and len(sys.argv) > 1 and sys.argv[1] == '--size-check':
        compare_model_sizes()
        print("\n=== ë‘ ëª¨ë¸ ëª¨ë‘ í›ˆë ¨ ===")
        
        # Normal DQN í›ˆë ¨
        print("\n1. ì¼ë°˜ DQN í›ˆë ¨ ì‹œì‘...")
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        train_agent(normal_agent, env, episodes=TRAIN_EPISODES, agent_name="Normal DQN")
        normal_agent.save(NORMAL_MODEL_PATH)
        
        # QAT DQN í›ˆë ¨
        print("\n2. QAT DQN í›ˆë ¨ ì‹œì‘...")
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        train_agent(qat_agent, env, episodes=TRAIN_EPISODES, agent_name="QAT DQN")
        qat_agent.save(QAT_MODEL_PATH)
        
        print("\në‘ ëª¨ë¸ í›ˆë ¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    elif args.mode == 'compare':
        print("\n=== ëª¨ë¸ ë¹„êµ ë¶„ì„ ===")
        
        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(NORMAL_MODEL_PATH) or not os.path.exists(QAT_MODEL_PATH):
            print("í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € train_both ëª¨ë“œë¡œ í›ˆë ¨ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            sys.exit(1)
        
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        normal_agent.load(NORMAL_MODEL_PATH)
        
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        qat_agent.load(QAT_MODEL_PATH)
        
        # ë¹„êµ ë¶„ì„ ì‹¤í–‰
        comparator = ModelComparator(normal_agent, qat_agent)
        comparator.generate_comprehensive_report(env)
        
    elif args.mode == 'simulate':
        print("\n=== ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ ===")
        
        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(NORMAL_MODEL_PATH) or not os.path.exists(QAT_MODEL_PATH):
            print("í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € train_both ëª¨ë“œë¡œ í›ˆë ¨ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            sys.exit(1)
        
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        normal_agent.load(NORMAL_MODEL_PATH)
        
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        qat_agent.load(QAT_MODEL_PATH)
        
        # ë Œë”ë§ ëª¨ë“œë¡œ í™˜ê²½ ì¬ìƒì„±
        env.close()
        env = gym.make('CartPole-v1', render_mode='human')
        
        # ì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        normal_data, qat_data = compare_live_simulations(normal_agent, qat_agent, env, episodes=3)
        
        print("\nì‹¤ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ê²°ê³¼ íŒŒì¼: {RESULTS_DIR}/live_simulation_comparison.png")
        
    elif args.mode == 'animate':
        print("\n=== ì• ë‹ˆë©”ì´ì…˜ ë¹„êµ ëª¨ë“œ ===")
        
        # ëª¨ë¸ ë¡œë“œ
        if not os.path.exists(NORMAL_MODEL_PATH) or not os.path.exists(QAT_MODEL_PATH):
            print("í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € train_both ëª¨ë“œë¡œ í›ˆë ¨ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            sys.exit(1)
        
        normal_agent = DQNAgent(state_size, action_size, use_qat=False)
        normal_agent.load(NORMAL_MODEL_PATH)
        
        qat_agent = DQNAgent(state_size, action_size, use_qat=True)
        qat_agent.load(QAT_MODEL_PATH)
        
        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± (ë Œë”ë§ ì—†ì´)
        print("ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„° ìƒì„± ì¤‘...")
        normal_data = run_live_simulation(normal_agent, env, "Normal DQN", episodes=1, delay=0)
        qat_data = run_live_simulation(qat_agent, env, "QAT DQN", episodes=1, delay=0)
        
        # ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        print("ì• ë‹ˆë©”ì´ì…˜ ìƒì„± ì¤‘...")
        anim = create_animated_comparison(normal_data, qat_data)
        
        print("ì• ë‹ˆë©”ì´ì…˜ ë¹„êµê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ê²°ê³¼ íŒŒì¼: {RESULTS_DIR}/cartpole_comparison.gif")
    
    env.close()
    print("\ní”„ë¡œê·¸ë¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ëª¨ë“  ê²°ê³¼ íŒŒì¼ì€ '{RESULTS_DIR}/' í´ë”ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
